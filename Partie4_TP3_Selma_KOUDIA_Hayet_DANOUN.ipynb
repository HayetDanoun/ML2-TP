{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bfad5897",
      "metadata": {
        "id": "bfad5897"
      },
      "source": [
        "# Partie 4 : Faites des tests avec deux autres couches de global pooling de Pytorch Geometric."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Travail à rendre :  (Applications pratiques des GNN) :**\n",
        "\n",
        "\n",
        "\n",
        "*   Important : Cette partie doit être rendue et sera évaluée.\n",
        "*   Deadline : Dimanche 26 janvier 2025 à 23h55.\n",
        "\n",
        "Dans cette partie du TP, vous êtes invités à identifier un cas concret d'application unique des réseaux de neurones de graphes (GNNs) et à démontrer comment les GNNs, en combinaison avec PyG, peuvent être utilisés pour résoudre ce problème. Le cas choisi doit être basé sur une problématique réelle et pertinente. Nous encourageons l'exploration de divers domaines (ex. : biologie, finance, réseaux sociaux), en vous inspirant des exemples illustrés dans l'enoncé de ce TP.\n",
        "\n",
        "Votre Notebook doit inclure une explication détaillée et accessible, pas uniquement du code. Veillez à :\n",
        "\n",
        "*  Inclure une introduction claire du cas d'utilisation\n",
        "*  Proposer des visualisations et des résultats interprétables\n",
        "*   Fournir des explications tout au long du notebook\n"
      ],
      "metadata": {
        "id": "Q1JYuSkQI_xZ"
      },
      "id": "Q1JYuSkQI_xZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Facebook"
      ],
      "metadata": {
        "id": "ZMd2KPHTc7mp"
      },
      "id": "ZMd2KPHTc7mp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Présentation du sujet"
      ],
      "metadata": {
        "id": "PN32QszgsDYT"
      },
      "id": "PN32QszgsDYT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La prédiction de liens est un problème clé en analyse de réseaux sociaux : étant donné la structure actuelle d'un réseau (par exemple Facebook, LinkedIn, etc.) et les attributs de ses nœuds, on veut estimer la probabilité qu'un lien (une amitié, un suivi, …) apparaisse entre deux nœuds qui ne sont pas encore connectés. C'est la base d'un moteur de recommandation d'amis ou de contacts.\n",
        "\n",
        "Dans ce mini-projet, nous allons :\n",
        "\n",
        "- Représenter chaque utilisateur comme un nœud du graphe.\n",
        "- Une arête (edge) indique une amitié existante entre deux utilisateurs.\n",
        "- Nous allons créer des « échantillons » positifs (des paires i,j qui ont déjà une arête) et négatifs (des paires i,j qui n'ont pas d'arête) pour entraîner un GNN à distinguer « existe un lien / n'existe pas de lien ».\n",
        "\n",
        "Par la suite, on pourra (dans un cas d'usage réel) recommander des liens à haute probabilité d'existence aux utilisateurs."
      ],
      "metadata": {
        "id": "Egmv358Qrcez"
      },
      "id": "Egmv358Qrcez"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Installation et imports"
      ],
      "metadata": {
        "id": "53gyN2CRr62-"
      },
      "id": "53gyN2CRr62-"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch_scatter torch_sparse torch_cluster torch_spline_conv \\\n",
        "    -f https://data.pyg.org/whl/torch-2.5.1+cu118.html\n",
        "%pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "# Pour la partie classification de graphes\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "\n",
        "# Pour évaluation link prediction\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Utilisons le GPU s'il est disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "ICl_8eN7rMRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5154d42d-114e-4b3c-eb19-6f3424fcc64d"
      },
      "id": "ICl_8eN7rMRE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu118.html\n",
            "Collecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_scatter-2.1.2%2Bpt25cu118-cp311-cp311-linux_x86_64.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_sparse-0.6.18%2Bpt25cu118-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_cluster-1.6.3%2Bpt25cu118-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu118/torch_spline_conv-1.2.2%2Bpt25cu118-cp311-cp311-linux_x86_64.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.9/950.9 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (1.26.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, torch_sparse, torch_cluster\n",
            "Successfully installed torch_cluster-1.6.3+pt25cu118 torch_scatter-2.1.2+pt25cu118 torch_sparse-0.6.18+pt25cu118 torch_spline_conv-1.2.2+pt25cu118\n",
            "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
            "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-g7bnufmk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-g7bnufmk\n",
            "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 9bffcd470057f2f865aedb87815267c243806e13\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.11.11)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric==2.7.0) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric==2.7.0) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric==2.7.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric==2.7.0) (2024.12.14)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.7.0-py3-none-any.whl size=1172888 sha256=649b509c00020425fcc0fed7b93174b887c51ae9b8adaaf6cd28a962a99699cd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-f8zs8r10/wheels/93/bb/85/bfec4ee59b2563f74ec87cc2c91c6a4d3e40d3dcdec8ee5afe\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:87: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:98: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:114: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:125: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Téléchargement et préparation du sous-ensemble Facebook"
      ],
      "metadata": {
        "id": "rcKfhfUusPQp"
      },
      "id": "rcKfhfUusPQp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette première partie, nous illustrons un cas d’usage réel d’un GNN : la prédiction de liens. L’idée est de recommander des amis potentiels dans un réseau social. Voici le pipeline :\n",
        "\n",
        "1. Charger un sous-ensemble Facebook (liste d’arêtes “qui est ami avec qui”).\n",
        "2. Construire un graphe PyG : Data(x, edge_index).\n",
        "3. Séparer les arêtes en train/val/test, plus negative sampling.\n",
        "4. Entraîner un GNN (GCNConv ou SAGEConv) pour obtenir des embeddings de nœuds.\n",
        "5. Prédire l’existence d’une arête via un dot product des embeddings."
      ],
      "metadata": {
        "id": "v4h6ob9sYjFV"
      },
      "id": "v4h6ob9sYjFV"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y57K4SeasLOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273b62dd-191e-48d8-ff13-ff2c653d4f2b"
      },
      "id": "y57K4SeasLOo",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ex.: un sous-réseau \"0.edges\" de l'archive facebook.tar.gz\n",
        "EDGE_FILE = '/content/drive/My Drive/facebook/0.edges'\n",
        "\n",
        "edges = []\n",
        "unique_nodes = set()\n",
        "with open(EDGE_FILE, 'r') as f:\n",
        "    for line in f:\n",
        "        src, dst = line.strip().split()\n",
        "        src = int(src)\n",
        "        dst = int(dst)\n",
        "        # Exclure self-loops éventuels\n",
        "        if src != dst:\n",
        "            edges.append((src, dst))\n",
        "            edges.append((dst, src))\n",
        "        unique_nodes.update([src, dst])\n",
        "\n",
        "unique_nodes = sorted(list(unique_nodes))\n",
        "node2idx = {nid: i for i, nid in enumerate(unique_nodes)}\n",
        "\n",
        "edge_index_list = []\n",
        "for (s,d) in edges:\n",
        "    edge_index_list.append([node2idx[s], node2idx[d]])\n",
        "edge_index = torch.tensor(edge_index_list, dtype=torch.long).t()  # shape [2, E]\n",
        "\n",
        "num_nodes = len(unique_nodes)\n",
        "print(f\"Nombre de nœuds : {num_nodes}\")\n",
        "print(f\"Nombre d'arêtes orientées : {edge_index.size(1)}\")\n",
        "\n",
        "# Pour simplifier, on se donne un vecteur x = one-hot(num_nodes)\n",
        "x = torch.eye(num_nodes)  # ou torch.ones((num_nodes,1))...\n",
        "\n",
        "data_facebook = Data(x=x, edge_index=edge_index).to(device)\n"
      ],
      "metadata": {
        "id": "cTBVe54BsiX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e979902-046d-4c29-9e86-4731c2033ea5"
      },
      "id": "cTBVe54BsiX4",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de nœuds : 333\n",
            "Nombre d'arêtes orientées : 10076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Création des paires positives / négatives pour la prédiction de liens"
      ],
      "metadata": {
        "id": "RSAPlJrvsMV3"
      },
      "id": "RSAPlJrvsMV3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée une liste “unique” d'arêtes (non orientées).\n",
        "Puis on la sépare en :\n",
        "\n",
        "  - 80 % pour train,\n",
        "  - 10 % pour val,\n",
        "  - 10 % pour test.\n",
        "\n",
        "Par la suite, pour entraîner un modèle de link prediction, on a besoin :\n",
        "\n",
        "- Paires positives : les arêtes existantes dans le graphe (en train).\n",
        "- Paires négatives : échantillons de nœuds non connectés.\n",
        "\n",
        "On crée autant de paires négatives (no link) que de paires positives."
      ],
      "metadata": {
        "id": "IBJJxmtPukEt"
      },
      "id": "IBJJxmtPukEt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraire la liste d'arêtes \"unique\" (s<d)\n",
        "unique_edges = []\n",
        "visited = set()\n",
        "E = edge_index.size(1)\n",
        "for i in range(0, E, 2):\n",
        "    s = edge_index[0, i].item()\n",
        "    d = edge_index[1, i].item()\n",
        "    if s > d:\n",
        "        s, d = d, s\n",
        "    if (s,d) not in visited:\n",
        "        visited.add((s,d))\n",
        "        unique_edges.append([s,d])\n",
        "\n",
        "unique_edges = np.array(unique_edges)\n",
        "np.random.shuffle(unique_edges)\n",
        "\n",
        "n_train = int(0.8 * len(unique_edges))\n",
        "n_val   = int(0.1 * len(unique_edges))\n",
        "\n",
        "train_pos = unique_edges[:n_train]\n",
        "val_pos   = unique_edges[n_train : n_train + n_val]\n",
        "test_pos  = unique_edges[n_train + n_val :]\n",
        "\n",
        "def negative_sampling(num_samples, N, forbidden_set):\n",
        "    \"\"\"Échantillonne des paires (s,d) non connectées dans [0..N-1]x[0..N-1].\"\"\"\n",
        "    neg = []\n",
        "    cpt = 0\n",
        "    while cpt < num_samples:\n",
        "        s = np.random.randint(0,N)\n",
        "        d = np.random.randint(0,N)\n",
        "        if s==d:\n",
        "            continue\n",
        "        if s>d:\n",
        "            s,d = d,s\n",
        "        if (s,d) in forbidden_set:\n",
        "            continue\n",
        "        neg.append([s,d])\n",
        "        # On marque cette paire comme \"prise\" pour ne pas la ré-échantillonner\n",
        "        forbidden_set.add((s,d))\n",
        "        cpt += 1\n",
        "    return np.array(neg)\n",
        "\n",
        "# On crée un set “positif” existant\n",
        "pos_set = set((s,d) for (s,d) in unique_edges)\n",
        "train_neg = negative_sampling(len(train_pos), num_nodes, pos_set)\n",
        "val_neg   = negative_sampling(len(val_pos),   num_nodes, pos_set)\n",
        "test_neg  = negative_sampling(len(test_pos),  num_nodes, pos_set)\n",
        "\n",
        "print(f\"Train pos : {len(train_pos)}, Train neg : {len(train_neg)}\")\n",
        "print(f\"Val   pos : {len(val_pos)},  Val   neg : {len(val_neg)}\")\n",
        "print(f\"Test  pos : {len(test_pos)}, Test  neg : {len(test_neg)}\")\n"
      ],
      "metadata": {
        "id": "9RoRM-tCss9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f147814b-a8e5-4221-9bc3-6432dae374a1"
      },
      "id": "9RoRM-tCss9L",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pos : 2015, Train neg : 2015\n",
            "Val   pos : 251,  Val   neg : 251\n",
            "Test  pos : 253, Test  neg : 253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour être rigoureux en link prediction, on peut retirer les arêtes val/test du edge_index avant d’entraîner, afin de ne pas “révéler” ces connexions. Ci-dessous, on crée un edge_index_train qui ne contient pas les arêtes de validation/test."
      ],
      "metadata": {
        "id": "F1s5Sfbyu0wK"
      },
      "id": "F1s5Sfbyu0wK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Supprimer val_pos et test_pos du graphe d'entraînement\n",
        "train_edges_set = set()\n",
        "for (s,d) in train_pos:\n",
        "    train_edges_set.add((s,d))\n",
        "    train_edges_set.add((d,s))\n",
        "\n",
        "# On reconstruit un edge_index réduit\n",
        "train_edge_list = []\n",
        "E = edge_index.size(1)\n",
        "for i in range(0, E):\n",
        "    s = edge_index[0, i].item()\n",
        "    d = edge_index[1, i].item()\n",
        "    if (s,d) in train_edges_set:\n",
        "        train_edge_list.append([s,d])\n",
        "\n",
        "edge_index_train = torch.tensor(train_edge_list, dtype=torch.long).t().to(device)\n",
        "print(\"Nbre d'arêtes dans edge_index_train :\", edge_index_train.size(1))\n",
        "\n",
        "# On crée un Data pour l'entraînement\n",
        "data_train = Data(x=data_facebook.x, edge_index=edge_index_train).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzq69ogrZZMS",
        "outputId": "f18859e5-dcbe-4367-fd69-e56a39815754"
      },
      "id": "Bzq69ogrZZMS",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nbre d'arêtes dans edge_index_train : 8060\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Définition du modèle GNN pour Link Prediction"
      ],
      "metadata": {
        "id": "bwhu4ofzt6j-"
      },
      "id": "bwhu4ofzt6j-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée :\n",
        "\n",
        "- Un encodeur GNN (GNNEncoder) basé sur SAGEConv ou GCNConv,\n",
        "- Un prédicteur LinkPredictor (ici un simple dot product),\n",
        "- Une classe Net pour assembler le tout et produire les logits sur des paires de nœuds."
      ],
      "metadata": {
        "id": "sIGQZ393u6d-"
      },
      "id": "sIGQZ393u6d-"
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dim, num_layers=2, model_type='SAGE'):\n",
        "        super().__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.model_type = model_type\n",
        "\n",
        "        if model_type == 'SAGE':\n",
        "            self.convs.append(SAGEConv(in_channels, hidden_dim))\n",
        "            for _ in range(num_layers-1):\n",
        "                self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
        "        elif model_type == 'GCN':\n",
        "            self.convs.append(GCNConv(in_channels, hidden_dim))\n",
        "            for _ in range(num_layers-1):\n",
        "                self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown model_type\")\n",
        "\n",
        "        self.dropout = 0.5\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        # Dernière couche sans relu\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sTBIThPRt3HJ"
      },
      "id": "sTBIThPRt3HJ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinkPredictor(torch.nn.Module):\n",
        "    \"\"\"Ici, on ne fait qu'un dot product entre les embeddings, sans MLP.\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        # Pour un simple dot product, pas besoin de plus de paramètres.\n",
        "        pass\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        return (x_i * x_j).sum(dim=-1)  # logit via dot product"
      ],
      "metadata": {
        "id": "DP2EJgSOuBxq"
      },
      "id": "DP2EJgSOuBxq",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dim, model_type='SAGE'):\n",
        "        super().__init__()\n",
        "        self.encoder = GNNEncoder(in_channels, hidden_dim, num_layers=2, model_type=model_type)\n",
        "        self.predictor = LinkPredictor(hidden_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        return self.encoder(data.x, data.edge_index)\n",
        "\n",
        "    def get_link_logits(self, node_emb, pairs):\n",
        "        x_i = node_emb[pairs[:,0]]\n",
        "        x_j = node_emb[pairs[:,1]]\n",
        "        return self.predictor(x_i, x_j)"
      ],
      "metadata": {
        "id": "D0hC8DdFZ8fJ"
      },
      "id": "D0hC8DdFZ8fJ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Entraînement et évaluation"
      ],
      "metadata": {
        "id": "qGJFFIVZuFaZ"
      },
      "id": "qGJFFIVZuFaZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons :\n",
        "\n",
        "1. Entraîner sur train_pos vs train_neg.\n",
        "2. Évaluer sur val_pos vs val_neg.\n",
        "3. Sélectionner le meilleur modèle sur la métrique ROC-AUC.\n",
        "4. Tester sur test_pos vs test_neg."
      ],
      "metadata": {
        "id": "fj8D2LVtwhHr"
      },
      "id": "fj8D2LVtwhHr"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, data_train, optimizer, train_pos, train_neg, batch_size=1024):\n",
        "    \"\"\"Apprend à distinguer train_pos vs. train_neg en BCE.\"\"\"\n",
        "    model.train()\n",
        "    pos_label = np.ones(len(train_pos), dtype=np.float32)\n",
        "    neg_label = np.zeros(len(train_neg), dtype=np.float32)\n",
        "\n",
        "    all_pairs = np.concatenate([train_pos, train_neg], axis=0)\n",
        "    all_labels = np.concatenate([pos_label, neg_label], axis=0)\n",
        "\n",
        "    # On mélange\n",
        "    idx = np.arange(len(all_pairs))\n",
        "    np.random.shuffle(idx)\n",
        "    all_pairs = torch.from_numpy(all_pairs[idx]).long().to(device)\n",
        "    all_labels = torch.from_numpy(all_labels[idx]).float().to(device)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    for start in range(0, len(all_pairs), batch_size):\n",
        "        end = start + batch_size\n",
        "        batch_pairs = all_pairs[start:end]\n",
        "        batch_labels = all_labels[start:end]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        node_emb = model(data_train)  # embeddings sur le graphe d'entraînement\n",
        "        logits = model.get_link_logits(node_emb, batch_pairs)\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * batch_pairs.size(0)\n",
        "    return total_loss / len(all_pairs)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model, data_full, pos_edges, neg_edges, batch_size=1024):\n",
        "    \"\"\"\n",
        "    Calcule l'AUC sur paires pos_edges vs. neg_edges.\n",
        "    data_full est le graphe complet (éventuellement on n'utilise que x,\n",
        "    mais on n'a pas besoin de ses arêtes sauf si on veut un autre embedding).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    node_emb = model(data_full)  # On peut extraire l'embedding sur tout le graphe\n",
        "\n",
        "    # On va concaténer pos et neg, calculer logits, comparer aux labels.\n",
        "    pairs = np.concatenate([pos_edges, neg_edges], axis=0)\n",
        "    labels = np.concatenate([np.ones(len(pos_edges)), np.zeros(len(neg_edges))], axis=0)\n",
        "\n",
        "    pairs = torch.from_numpy(pairs).long().to(device)\n",
        "    logits = model.get_link_logits(node_emb, pairs).cpu().numpy()\n",
        "    # Sigmoid pour obtenir un score [0..1]\n",
        "    scores = 1/(1+np.exp(-logits))\n",
        "\n",
        "    auc_val = roc_auc_score(labels, scores)\n",
        "    return auc_val\n"
      ],
      "metadata": {
        "id": "HOP91eNQuGc3"
      },
      "id": "HOP91eNQuGc3",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(in_channels=data_facebook.x.size(1), hidden_dim=64, model_type='SAGE').to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "best_val_auc = 0\n",
        "best_state = None\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    loss = train_one_epoch(model, data_train, optimizer, train_pos, train_neg, batch_size=512)\n",
        "    val_auc = eval_model(model, data_facebook, val_pos, val_neg)\n",
        "\n",
        "    if val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        best_state = {k: v.cpu() for k,v in model.state_dict().items()}\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"[Epoch {epoch:02d}] loss={loss:.4f} val_auc={val_auc:.4f}\")\n",
        "\n",
        "# Charger le meilleur\n",
        "model.load_state_dict({k: v.to(device) for k,v in best_state.items()})\n",
        "test_auc = eval_model(model, data_facebook, test_pos, test_neg)\n",
        "print(f\"Meilleur modèle : val_auc={best_val_auc:.4f} => test_auc={test_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXjP72xIuJQj",
        "outputId": "53255679-4fb7-4fad-f02e-bffb37466f7f"
      },
      "id": "TXjP72xIuJQj",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 05] loss=0.4606 val_auc=0.9245\n",
            "[Epoch 10] loss=0.3884 val_auc=0.9297\n",
            "[Epoch 15] loss=0.3459 val_auc=0.9195\n",
            "[Epoch 20] loss=0.3014 val_auc=0.9050\n",
            "[Epoch 25] loss=0.2717 val_auc=0.8986\n",
            "[Epoch 30] loss=0.2546 val_auc=0.8840\n",
            "Meilleur modèle : val_auc=0.9338 => test_auc=0.8825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On obtient un score AUC mesurant la capacité du modèle à distinguer les paires connectées vs. non connectées (plus l'AUC est proche de 1, meilleure est la performance)."
      ],
      "metadata": {
        "id": "L-2RdvPuwwMq"
      },
      "id": "L-2RdvPuwwMq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Les enzymes"
      ],
      "metadata": {
        "id": "p8i9xaBmakHe"
      },
      "id": "p8i9xaBmakHe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans cette deuxieme partie nous allons répondre à la Partie 4 du TP : « tester deux couches de global pooling », nous illustrons ici un autre problème : la classification de graphes.\n",
        "\n",
        "Pour se faire, nous allons utiliser un dataset TUDataset (par exemple ENZYMES), l'idée est :\n",
        "\n",
        "- Chaque exemple est un petit graphe.\n",
        "- On veut prédire la classe ou le label associé à ce graphe (ex. type d'enzyme).\n",
        "- Pour agréger l’information des nœuds en un seul vecteur de graphe, on compare :\n",
        "        global_mean_pool vs. global_add_pool."
      ],
      "metadata": {
        "id": "vJiq-Aaoao1Q"
      },
      "id": "vJiq-Aaoao1Q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Chargement du dataset"
      ],
      "metadata": {
        "id": "yuPzwUMlbFPu"
      },
      "id": "yuPzwUMlbFPu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, on prend le dataset ENZYMES, qui contient 600 graphes, chacun étiqueté en 6 classes."
      ],
      "metadata": {
        "id": "nIlbbc06bLcL"
      },
      "id": "nIlbbc06bLcL"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "# root = './mutag_data'\n",
        "# name = 'MUTAG'\n",
        "\n",
        "root = './enzymes_data'\n",
        "name = 'ENZYMES'\n",
        "dataset_enz = TUDataset(root=root, name=name)\n",
        "\n",
        "print(dataset_enz)\n",
        "print(f\"Nombre de graphes = {len(dataset_enz)}\")\n",
        "print(f\"Nombre de classes = {dataset_enz.num_classes}\")\n",
        "print(f\"Dimension des features nœuds = {dataset_enz.num_features}\")\n",
        "\n",
        "# Pour la démo, on fait un split train/val/test (60/20/20)\n",
        "nb_graphs = len(dataset_enz)\n",
        "indices = list(range(nb_graphs))\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.2\n",
        "ntrain = int(train_ratio*nb_graphs)\n",
        "nval   = int(val_ratio*nb_graphs)\n",
        "\n",
        "train_idx = indices[:ntrain]\n",
        "val_idx   = indices[ntrain : ntrain+nval]\n",
        "test_idx  = indices[ntrain+nval:]\n",
        "\n",
        "train_dataset = dataset_enz[train_idx]\n",
        "val_dataset   = dataset_enz[val_idx]\n",
        "test_dataset  = dataset_enz[test_idx]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AddEgRgajnH",
        "outputId": "79853816-2ec6-487d-9a35-0de9fa5a3b2b"
      },
      "id": "2AddEgRgajnH",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES(600)\n",
            "Nombre de graphes = 600\n",
            "Nombre de classes = 6\n",
            "Dimension des features nœuds = 3\n",
            "Train: 360, Val: 120, Test: 120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Définition d'un GNN pour classification de graphes"
      ],
      "metadata": {
        "id": "gFIVTDmMbP3g"
      },
      "id": "gFIVTDmMbP3g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous réutilisons un GCN + un pooling global + un linear final.\n",
        "0n rend le pooling paramétrable (pooling_type), et on va tester mean vs. add."
      ],
      "metadata": {
        "id": "A9Hvl7c1bVem"
      },
      "id": "A9Hvl7c1bVem"
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN_Graph(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, num_layers, dropout, pooling_type='mean'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Liste de couches GCNConv\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_dim, hidden_dim))\n",
        "        for _ in range(num_layers-2):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "        self.convs.append(GCNConv(hidden_dim, out_dim))  # sortie = out_dim\n",
        "\n",
        "        # Liste de BatchNorm (pour les couches intermédiaires)\n",
        "        self.bns = torch.nn.ModuleList([\n",
        "            torch.nn.BatchNorm1d(hidden_dim) for _ in range(num_layers-1)\n",
        "        ])\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Pooling global : add ou mean\n",
        "        if pooling_type=='add':\n",
        "            self.pool = global_add_pool\n",
        "        else:\n",
        "            # défaut = mean\n",
        "            self.pool = global_mean_pool\n",
        "\n",
        "    def forward(self, batch_data):\n",
        "        x, edge_index, batch = batch_data.x, batch_data.edge_index, batch_data.batch\n",
        "\n",
        "        for i in range(len(self.convs)-1):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = self.bns[i](x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # dernière couche sans BN ni relu, par ex.\n",
        "        x = self.convs[-1](x, edge_index)\n",
        "\n",
        "        # on obtient des features par nœud => pooling global\n",
        "        graph_emb = self.pool(x, batch)\n",
        "        return graph_emb\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n"
      ],
      "metadata": {
        "id": "jIKPQKdubSc2"
      },
      "id": "jIKPQKdubSc2",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous pouvons soit incorporer la projection finale dans la dernière couche du GCN (c'est déjà out_dim = num_classes), soit ajouter un Linear. Ici, comme nous avons mis out_dim = num_classes, il suffit de faire un LogSoftmax sur le vecteur agrégé."
      ],
      "metadata": {
        "id": "DvDsHU0Rbdr_"
      },
      "id": "DvDsHU0Rbdr_"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        emb = model(batch)  # shape [batch_size, num_classes]\n",
        "        # Le label est batch.y (shape [batch_size]) => cross-entropy\n",
        "        loss = criterion(emb, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch.num_graphs\n",
        "        total_examples += batch.num_graphs\n",
        "    return total_loss / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model_classif(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch)  # [batch_size, num_classes]\n",
        "        pred = out.argmax(dim=-1)  # classes\n",
        "        correct += (pred == batch.y).sum().item()\n",
        "        total += batch.num_graphs\n",
        "    return correct / total\n"
      ],
      "metadata": {
        "id": "Vr4w8aVqbgO5"
      },
      "id": "Vr4w8aVqbgO5",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Comparaison pooling=mean vs. pooling=add"
      ],
      "metadata": {
        "id": "stSiwQX8blfp"
      },
      "id": "stSiwQX8blfp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entraînons deux fois le même GNN, juste en changeant le type de pooling."
      ],
      "metadata": {
        "id": "2Mdhs0Lrbn4s"
      },
      "id": "2Mdhs0Lrbn4s"
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(pooling_type='mean'):\n",
        "    model = GCN_Graph(\n",
        "        in_dim=dataset_enz.num_features,\n",
        "        hidden_dim=64,\n",
        "        out_dim=dataset_enz.num_classes,\n",
        "        num_layers=3,\n",
        "        dropout=0.5,\n",
        "        pooling_type=pooling_type\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    best_val_acc = 0\n",
        "    best_state = None\n",
        "    nb_epochs = 30\n",
        "\n",
        "    for epoch in range(1, nb_epochs+1):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        train_acc = eval_model_classif(model, train_loader)\n",
        "        val_acc   = eval_model_classif(model, val_loader)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = {k: v.cpu() for k,v in model.state_dict().items()}\n",
        "\n",
        "        if epoch%10==0:\n",
        "            print(f\"[{pooling_type}] Epoch {epoch:02d} | loss={loss:.4f} | \"\n",
        "                  f\"train_acc={train_acc:.3f} | val_acc={val_acc:.3f}\")\n",
        "\n",
        "    # On recharge le meilleur\n",
        "    model.load_state_dict({k: v.to(device) for k,v in best_state.items()})\n",
        "    test_acc = eval_model_classif(model, test_loader)\n",
        "    print(f\"=> Pooling={pooling_type}, Best val_acc={best_val_acc:.3f}, Test acc={test_acc:.3f}\")\n",
        "    return test_acc\n",
        "\n",
        "print(\"=== Test 1 : global_mean_pool ===\")\n",
        "test_acc_mean = run_experiment(pooling_type='mean')\n",
        "\n",
        "print(\"\\n=== Test 2 : global_add_pool ===\")\n",
        "test_acc_add = run_experiment(pooling_type='add')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgOtbv3Tboh8",
        "outputId": "c15695bc-30f2-43a6-89dc-d411a25719cf"
      },
      "id": "RgOtbv3Tboh8",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Test 1 : global_mean_pool ===\n",
            "[mean] Epoch 10 | loss=1.7139 | train_acc=0.331 | val_acc=0.242\n",
            "[mean] Epoch 20 | loss=1.7050 | train_acc=0.325 | val_acc=0.300\n",
            "[mean] Epoch 30 | loss=1.6815 | train_acc=0.250 | val_acc=0.233\n",
            "=> Pooling=mean, Best val_acc=0.300, Test acc=0.250\n",
            "\n",
            "=== Test 2 : global_add_pool ===\n",
            "[add] Epoch 10 | loss=4.8810 | train_acc=0.236 | val_acc=0.225\n",
            "[add] Epoch 20 | loss=2.5756 | train_acc=0.289 | val_acc=0.208\n",
            "[add] Epoch 30 | loss=2.1003 | train_acc=0.236 | val_acc=0.175\n",
            "=> Pooling=add, Best val_acc=0.283, Test acc=0.208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les résultats montrent que l'utilisation de global_mean_pool offre une meilleure précision sur le test que global_add_pool pour la classification de graphes, ce qui suggère que l'agrégation moyenne peut capturer les caractéristiques des graphes de manière plus efficace dans ce contexte particulier. Toutefois, la performance peut varier en fonction du dataset et de l'architecture utilisée."
      ],
      "metadata": {
        "id": "fr1S0CmccjXr"
      },
      "id": "fr1S0CmccjXr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "URvjAnEkbxFY"
      },
      "id": "URvjAnEkbxFY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Link Prediction (Partie : Facebook)**\n",
        "- Nous avons illustré un cas d'usage concret des GNN : la recommandation d'amis.\n",
        "- Le modèle GNN encode chaque utilisateur via SAGEConv/GCNConv, puis un dot product estime la probabilité de lien.\n",
        "- Nous avons évalué la qualité de prédiction via l'AUC sur paires positives/négatives.\n",
        "\n",
        "Classification de graphes (Partie: ENZYMES)\n",
        "- Nous avons construit un GCN pour classifier des graphes ENZYMES, avec un pooling global.\n",
        "- Nous avons comparé global_mean_pool et global_add_pool.\n",
        "- Les performances peuvent différer selon la nature du dataset et l'architecture."
      ],
      "metadata": {
        "id": "w-fETu1icGNH"
      },
      "id": "w-fETu1icGNH"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}